{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hazm import *\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## Second Part of Exercise\n",
    "class LanguageModel():\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_corpus_sentences(direction):\n",
    "        sentences = None\n",
    "        with open(direction,'r') as input_file:\n",
    "            sentences = input_file.readlines()\n",
    "        return sentences\n",
    "    \n",
    "    def __init__(self,n,smoothing,corpus_dir):\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.corpus_dir = corpus_dir        \n",
    "        \n",
    "    def add_unigram(self,token):\n",
    "        self.unigram_count[token] += 1\n",
    "        self.total_tokens += 1\n",
    "        \n",
    "    def train_unigram(self):\n",
    "        self.unigram_count, sentences, self.total_tokens, self.best_next = \\\n",
    "        defaultdict(lambda: 0), self.get_corpus_sentences(self.corpus_dir), 0,defaultdict(lambda: 'UNK')\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\" \")\n",
    "            for token in tokens:\n",
    "                self.add_unigram(token)\n",
    "                \n",
    "    def smoothed_unigram(self, token_count):\n",
    "        if self.smoothing == False:\n",
    "            return token_count/self.total_tokens\n",
    "        else:\n",
    "            return (token_count + 1) / (self.total_tokens + len(self.unigram_count))\n",
    "        \n",
    "    def unigram_probability(self,sentence):\n",
    "        tokens, log_of_probability = sentence.split(\" \"), 0\n",
    "        for token in tokens:\n",
    "            log_of_probability += np.log10(self.smoothed_unigram(self.unigram_count[token]))\n",
    "        return log_of_probability\n",
    "#########################################################    Second Part of Exercise\n",
    "\n",
    "    def add_bigram(self,first_token, second_token):\n",
    "        self.bigram_count[first_token][second_token] += 1\n",
    "        self.bigram_continuation[second_token].add(first_token)\n",
    "            \n",
    "    def bigram_find_best_next(self):\n",
    "        for token in self.bigram_count.keys():\n",
    "            best_token, best_count = 'A', 0\n",
    "            for next_token in self.bigram_count[token]:\n",
    "                self.distinct_word_bigrams += 1\n",
    "                if self.bigram_count[token][next_token] > best_count:\n",
    "                    best_token, best_count = next_token, self.bigram_count[token][next_token]\n",
    "            self.bigram_next_best[token] = best_token\n",
    "        \n",
    "    def train_bigram(self):\n",
    "        self.bigram_next_best = defaultdict(lambda: 'UNK')\n",
    "        self.bigram_count,self.distinct_word_bigrams = defaultdict(lambda: defaultdict(lambda: 0)), 0\n",
    "        self.bigram_continuation, sentences = defaultdict(lambda: set()),self.get_corpus_sentences(self.corpus_dir)        \n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            for pointer in range(1,len(tokens)):\n",
    "                first_token, second_token = tokens[pointer-1], tokens[pointer]\n",
    "                self.add_bigram(first_token, second_token)\n",
    "        self.bigram_find_best_next()\n",
    "                               \n",
    "    def smoothed_bigram(self,first_token,second_token):\n",
    "        if self.smoothing == False:  \n",
    "            if self.unigram_count[first_token] == 0:\n",
    "                return 0\n",
    "            return (self.bigram_count[first_token][second_token] / self.unigram_count[first_token])\n",
    "            \n",
    "        elif self.smoothing == \"laplace\":\n",
    "            return (self.bigram_count[first_token][second_token]+1)/(self.unigram_count[first_token]+len(self.unigram_count))      \n",
    "        ########################################## Bonus Part (Kneser-ney) for bigrams   \n",
    "        elif self.smoothing == 'kneser-ney':\n",
    "            tmp = self.unigram_count[first_token] * self.distinct_word_bigrams * self.unigram_count[first_token]\n",
    "            if tmp == 0:\n",
    "                return 0       \n",
    "            DISCOUNT = 0.75\n",
    "            LAMBDA = (DISCOUNT * len(self.bigram_count[first_token]))/(self.unigram_count[first_token])\n",
    "            P_CONTINUATION = (len(self.bigram_continuation[second_token]))/(self.distinct_word_bigrams)\n",
    "            DISCOUNTED_BIGRAM = (max(self.bigram_count[first_token][second_token]-DISCOUNT, 0))\n",
    "            BIGRAM_PART = DISCOUNTED_BIGRAM / (self.unigram_count[first_token])\n",
    "            return (BIGRAM_PART + (LAMBDA*P_CONTINUATION))\n",
    "        \n",
    "    def bigram_probability(self, sentence):\n",
    "        tokens = sentence.split(\" \")\n",
    "        log_of_probability = np.log10(self.smoothed_unigram(self.unigram_count[tokens[0]]))\n",
    "        for pointer in range(1,len(tokens)):\n",
    "            first_token, second_token = tokens[pointer-1], tokens[pointer]\n",
    "            log_of_probability += np.log10(self.smoothed_bigram(first_token,second_token))\n",
    "        return log_of_probability\n",
    "    \n",
    "##########################################################  Bonus Part ( Trigrams )\n",
    "    def add_trigram(self,first_token, middle_token, last_token):\n",
    "        self.trigram_count[first_token][middle_token][last_token] += 1\n",
    "        pair = (first_token,middle_token)\n",
    "        self.trigram_continuation[last_token].add(pair)\n",
    "    \n",
    "    def trigram_find_next_best(self):\n",
    "        for token in self.trigram_count.keys():\n",
    "            for next_token in self.trigram_count[token]:\n",
    "                best_token, best_count = 'A', 0\n",
    "                for second_next_token in self.trigram_count[token][next_token].keys():\n",
    "                    self.distinct_word_trigrams += 1\n",
    "                    if self.trigram_count[token][next_token][second_next_token] > best_count:\n",
    "                        best_token, best_count = second_next_token, self.trigram_count[token][next_token][second_next_token]\n",
    "                self.trigram_next_best[(token,next_token)] = best_token\n",
    "                  \n",
    "    def train_trigram(self):\n",
    "        self.trigram_next_best = defaultdict(lambda: 'UNK')\n",
    "        sentences, self.trigram_continuation = self.get_corpus_sentences(self.corpus_dir), defaultdict(lambda: set())\n",
    "        self.trigram_count, self.distinct_word_trigrams = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0))), 0\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            for pointer in range(2,len(tokens)):\n",
    "                first_token, middle_token, last_token = tokens[pointer-2], tokens[pointer-1], tokens[pointer]\n",
    "                self.add_trigram(first_token, middle_token,last_token)\n",
    "        self.trigram_find_next_best()\n",
    "        \n",
    "    def smoothed_trigram(self,first_token, middle_token, last_token):\n",
    "        if self.smoothing == False:\n",
    "            if self.bigram_count[first_token][middle_token] == 0:\n",
    "                return 0\n",
    "            return (self.trigram_count[first_token][middle_token][last_token] / self.bigram_count[first_token][middle_token])\n",
    "                    \n",
    "        elif self.smoothing == 'laplace':\n",
    "            return (self.trigram_count[first_token][middle_token][last_token] + 1) / (self.bigram_count[first_token][middle_token] + len(self.unigram_count))\n",
    "        ########################################## Bonus Part (kneser-ney for trigrams)\n",
    "        elif self.smoothing == 'kneser-ney':\n",
    "            tmp = self.bigram_count[first_token][middle_token] * self.distinct_word_trigrams * self.bigram_count[first_token][middle_token]\n",
    "            if tmp == 0:\n",
    "                return 0\n",
    "            DISCOUNT = 0.75\n",
    "            LAMBDA = (DISCOUNT * len(self.trigram_count[first_token][middle_token]))/(self.bigram_count[first_token][middle_token])\n",
    "            P_CONTINUATION = (len(self.trigram_continuation[last_token]))/(self.distinct_word_trigrams)\n",
    "            DISCOUNTED_TRIGRAM = (max(self.trigram_count[first_token][middle_token][last_token]-DISCOUNT, 0))\n",
    "            TRIGRAM_PART = DISCOUNTED_TRIGRAM / (self.bigram_count[first_token][middle_token])\n",
    "            return (TRIGRAM_PART + (LAMBDA*P_CONTINUATION))\n",
    "        \n",
    "    def trigram_probability(self, sentence):\n",
    "        tokens = sentence.split(\" \")\n",
    "        log_of_probability = np.log10(self.smoothed_unigram(self.unigram_count[tokens[0]])) + \\\n",
    "                             np.log10(self.smoothed_bigram(tokens[0],tokens[1]))\n",
    "        for pointer in range(2,len(tokens)):\n",
    "            first_token, middle_token, last_token = tokens[pointer-2], tokens[pointer-1], tokens[pointer]\n",
    "            log_of_probability += np.log10(self.smoothed_trigram(first_token,middle_token,last_token))\n",
    "        return log_of_probability\n",
    "\n",
    "\n",
    "\n",
    "##########################################################     Second part of Exercise    \n",
    "    def train(self):\n",
    "        if self.n >= 1:\n",
    "            self.train_unigram()\n",
    "        if self.n >= 2:\n",
    "            self.train_bigram()\n",
    "        if self.n >= 3:\n",
    "            self.train_trigram()\n",
    "            \n",
    "    def prob(self, sentence):\n",
    "        if self.n == 1:\n",
    "            return self.unigram_probability(sentence)\n",
    "        elif self.n == 2:\n",
    "            return self.bigram_probability(sentence)\n",
    "        else:\n",
    "            return self.trigram_probability(sentence)\n",
    "##########################################################    Second part of Exercise\n",
    "\n",
    "    def unigram_generate(self, sentence):\n",
    "        # Higher Frequency ----> higher probability\n",
    "        MINUS_INF = -1000000\n",
    "        best_token, highest_frequency = None, MINUS_INF\n",
    "        return self.best_next['UNK']\n",
    "    \n",
    "    def bigram_generate(self,sentence):\n",
    "        tokens, MINUS_INF = sentence.split(\" \"), -1000000 \n",
    "        last_token = tokens[-1]\n",
    "        if len(tokens) == 0:\n",
    "            return 'UNK'\n",
    "        if last_token != '</s>':\n",
    "            best_next = self.bigram_next_best[last_token]\n",
    "            return best_next\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    def trigram_generate(self,sentence):\n",
    "        tokens, MINUS_INF = sentence.split(\" \"), -1000000 \n",
    "        if len(tokens) <= 1:\n",
    "            return 'UNK'       \n",
    "        second_last_token, last_token = tokens[-2], tokens[-1]\n",
    "        if last_token == '</s>':\n",
    "            return ''\n",
    "        best_next = self.trigram_next_best[(second_last_token,last_token)]\n",
    "        return best_next\n",
    "\n",
    "    def generate(self, sentence):\n",
    "        if self.n == 1:\n",
    "            return self.unigram_generate(sentence)\n",
    "        elif self.n == 2:\n",
    "            return self.bigram_generate(sentence)\n",
    "        elif self.n == 3:\n",
    "            return self.trigram_generate(sentence)\n",
    "        \n",
    "######################################################### Third Part of Exercise\n",
    "    @staticmethod    \n",
    "    def wer_calculator(original_sentence, generated_sentence):\n",
    "        original, generated = original_sentence.split(\" \"), generated_sentence.split(\" \")\n",
    "        N, M = len(original), len(generated)\n",
    "        matrix = [[0 for j in range(N+1)] for i in range(M+1)]\n",
    "        for j in range(N+1):\n",
    "            matrix[0][j] = j\n",
    "        for i in range(M+1):\n",
    "            matrix[i][0] = i\n",
    "        for i in range(1,M+1):\n",
    "            for j in range(1,N+1):\n",
    "                if original[j-1] == generated[i-1]:\n",
    "                    matrix[i][j] = matrix[i-1][j-1]\n",
    "                else:\n",
    "                    matrix[i][j] = min(min(matrix[i-1][j],matrix[i][j-1]),matrix[i-1][j-1]) + 1 \n",
    "        return ((matrix[M][N])/N)\n",
    "            \n",
    "        \n",
    "        \n",
    "##############################################################  Third Part of Exercise\n",
    "\n",
    "\n",
    "    def evaluate(self,dir):\n",
    "        validation_sentences = []\n",
    "        with open(dir, 'r', encoding = 'utf-8') as data:\n",
    "            validation_sentences = data.readlines()\n",
    "        average_wer = 0\n",
    "        for sentence, index in zip(validation_sentences,range(len(validation_sentences))):\n",
    "            tmp = sentence.split(\" \")\n",
    "            generated_sentence = tmp[0]+ \" \" + tmp[1]\n",
    "            LINE_AVG_TOKENS = 35\n",
    "            for i in range(LINE_AVG_TOKENS):\n",
    "                next_token = self.generate(generated_sentence)\n",
    "                generated_sentence += \" \" + next_token      \n",
    "                if next_token == \"</s>\":\n",
    "                    break\n",
    "            edited_sentence = sentence.replace(\" \\n\",'')\n",
    "            tmp1 = self.wer_calculator(edited_sentence, generated_sentence)\n",
    "            if index % 25000 == 0:\n",
    "                print(\"document index:\", index, \"Processing\")\n",
    "            average_wer += tmp1\n",
    "        return (average_wer/len(validation_sentences))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Every Possible Model + Trigram Models (Bonus Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.703579902648926\n"
     ]
    }
   ],
   "source": [
    "# Expected to last in 17 Seconds with my configurations\n",
    "a = time.time()\n",
    "lm1 = LanguageModel(1,False,'sentences.txt')\n",
    "lm1.train()\n",
    "b = time.time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.4784722328186\n"
     ]
    }
   ],
   "source": [
    "# Expected to last in 60 Seconds with my configurations\n",
    "a = time.time()\n",
    "lm2 = LanguageModel(2,False,'sentences.txt')\n",
    "lm2.train()\n",
    "b = time.time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164.98213505744934\n"
     ]
    }
   ],
   "source": [
    "# Expected to last in 170 Seconds with my configurations\n",
    "a = time.time()\n",
    "lm3 = LanguageModel(3,False,'sentences.txt')\n",
    "lm3.train()\n",
    "b = time.time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.036351919174194\n"
     ]
    }
   ],
   "source": [
    "# Expected to last in 17.5 Seconds with my configurations\n",
    "a = time.time()\n",
    "lm4 = LanguageModel(1,'laplace','sentences.txt')\n",
    "lm4.train()\n",
    "b = time.time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.58028435707092\n"
     ]
    }
   ],
   "source": [
    "# Expected to last in 60 Seconds with my configurations\n",
    "a = time.time()\n",
    "lm5 = LanguageModel(2,'laplace','sentences.txt')\n",
    "lm5.train()\n",
    "b = time.time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.35599637031555\n"
     ]
    }
   ],
   "source": [
    "# Expected to last in 175 Seconds with my configurations\n",
    "a = time.time()\n",
    "lm6 = LanguageModel(3,'laplace','sentences.txt')\n",
    "lm6.train()\n",
    "b = time.time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kneser-ney models: Bonus Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.16614890098572\n"
     ]
    }
   ],
   "source": [
    "# Expected to last in 60 Seconds with my configurations\n",
    "a = time.time()\n",
    "lm7 = LanguageModel(2,'kneser-ney','sentences.txt')\n",
    "lm7.train()\n",
    "b = time.time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.9487099647522\n"
     ]
    }
   ],
   "source": [
    "# Expected to last in 190 Seconds with my configurations\n",
    "a = time.time()\n",
    "lm8 = LanguageModel(3,'kneser-ney','sentences.txt')\n",
    "lm8.train()\n",
    "b = time.time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Check, 2nd part of Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-36.63284375127532\n",
      "-13.39824125501898\n",
      "-13.39824125501898\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehrad/Programmings/NLP/lib/python3.7/site-packages/ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    }
   ],
   "source": [
    "print(lm1.prob(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\"))\n",
    "print(lm1.prob('<s> منابع انتهای آب پیام'))\n",
    "print(lm1.prob('<s> انتهای پیام منابع آب'))\n",
    "print(lm1.prob('<s> hello انتهای پیام منابع آب'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.19834821806493\n",
      "-9.224401565757852\n",
      "-3.2090128608120887\n",
      "-inf\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehrad/Programmings/NLP/lib/python3.7/site-packages/ipykernel_launcher.py:89: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    }
   ],
   "source": [
    "print(lm2.prob(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\"))\n",
    "print(lm2.prob('<s> منابع آب </s>'))\n",
    "print(lm2.prob('<s> انتهای پیام </s>'))\n",
    "print(lm2.prob('<s> منابع انتهای آب پیام </s>'))\n",
    "print(lm2.prob('<s> منابع انتهای آب پیام hello </s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.47344270965778\n",
      "-9.86393880978254\n",
      "-3.1939508020136396\n",
      "-inf\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehrad/Programmings/NLP/lib/python3.7/site-packages/ipykernel_launcher.py:145: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    }
   ],
   "source": [
    "print(lm3.prob(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\"))\n",
    "print(lm3.prob('<s> منابع آب </s>'))\n",
    "print(lm3.prob('<s> انتهای پیام </s>'))\n",
    "print(lm3.prob('<s> منابع انتهای آب پیام </s>'))\n",
    "print(lm3.prob('<s> منابع انتهای آب پیام hello </s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-36.63361422312002\n",
      "-13.398762013493927\n",
      "-13.398762013493927\n",
      "-20.97056040784843\n"
     ]
    }
   ],
   "source": [
    "print(lm4.prob(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\"))\n",
    "print(lm4.prob('<s> منابع انتهای آب پیام'))\n",
    "print(lm4.prob('<s> انتهای پیام منابع آب'))\n",
    "print(lm4.prob('<s> hello انتهای پیام منابع آب'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21.716622846982787\n",
      "-9.573371875605977\n",
      "-3.3128357779750166\n",
      "-19.043625247465844\n",
      "-27.52994227066596\n"
     ]
    }
   ],
   "source": [
    "print(lm5.prob(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\"))\n",
    "print(lm5.prob('<s> منابع آب </s>'))\n",
    "print(lm5.prob('<s> انتهای پیام </s>'))\n",
    "print(lm5.prob('<s> منابع انتهای آب پیام </s>'))\n",
    "print(lm5.prob('<s> منابع انتهای آب پیام hello </s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24.60552115588222\n",
      "-12.032312795207122\n",
      "-3.311747781430808\n",
      "-20.646374473956733\n",
      "-24.6432799846524\n"
     ]
    }
   ],
   "source": [
    "print(lm6.prob(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\"))\n",
    "print(lm6.prob('<s> منابع آب </s>'))\n",
    "print(lm6.prob('<s> انتهای پیام </s>'))\n",
    "print(lm6.prob('<s> منابع انتهای آب پیام </s>'))\n",
    "print(lm6.prob('<s> منابع انتهای آب پیام hello </s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.22244733084632\n",
      "-9.210889145950755\n",
      "-3.2090991319216156\n",
      "-20.25915384423504\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehrad/Programmings/NLP/lib/python3.7/site-packages/ipykernel_launcher.py:89: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    }
   ],
   "source": [
    "print(lm7.prob(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\"))\n",
    "print(lm7.prob('<s> منابع آب </s>'))\n",
    "print(lm7.prob('<s> انتهای پیام </s>'))\n",
    "print(lm7.prob('<s> منابع انتهای آب پیام </s>'))\n",
    "print(lm7.prob('<s> منابع انتهای آب پیام hello </s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.732442248174511\n",
      "-10.165538609849596\n",
      "-3.1940289229012264\n",
      "-inf\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehrad/Programmings/NLP/lib/python3.7/site-packages/ipykernel_launcher.py:145: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    }
   ],
   "source": [
    "print(lm8.prob(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\"))\n",
    "print(lm8.prob('<s> منابع آب </s>'))\n",
    "print(lm8.prob('<s> انتهای پیام </s>'))\n",
    "print(lm8.prob('<s> منابع انتهای آب پیام </s>'))\n",
    "print(lm8.prob('<s> منابع انتهای آب پیام hello </s>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Check, 2nd part of Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm1.generate(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'گزارش'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.generate(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'بازیکنانم'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.generate(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm4.generate(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'گزارش'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm5.generate(\"به گزارش گروه ورزش باشگاه خبرنگاران جوان بر اساس برنامه‌ای که برانکو به\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm1.generate(\"<s> انتهای\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'پیام'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.generate(\"<s> انتهای\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'پیام'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.generate(\"<s> انتهای\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models, 3rd Part of Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document index: 0 Processing\n",
      "document index: 25000 Processing\n",
      "document index: 50000 Processing\n",
      "document index: 75000 Processing\n",
      "document index: 100000 Processing\n",
      "document index: 125000 Processing\n",
      "document index: 150000 Processing\n",
      "average WER:  1.885516606406248\n",
      "duration: 152.14761900901794\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "result = lm1.evaluate('/Users/mehrad/programmings/nlp/codes/phase1/validation_sentences.txt')\n",
    "print(\"average WER: \", result)\n",
    "b = time.time()\n",
    "print(\"duration:\",b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document index: 0 Processing\n",
      "document index: 25000 Processing\n",
      "document index: 50000 Processing\n",
      "document index: 75000 Processing\n",
      "document index: 100000 Processing\n",
      "document index: 125000 Processing\n",
      "document index: 150000 Processing\n",
      "average WER:  1.3448067444020273\n",
      "duration: 149.62035393714905\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "result = lm2.evaluate('/Users/mehrad/programmings/nlp/codes/phase1/validation_sentences.txt')\n",
    "print(\"average WER: \", result)\n",
    "b = time.time()\n",
    "print(\"duration:\",b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document index: 0 Processing\n",
      "document index: 25000 Processing\n",
      "document index: 50000 Processing\n",
      "document index: 75000 Processing\n",
      "document index: 100000 Processing\n",
      "document index: 125000 Processing\n",
      "document index: 150000 Processing\n",
      "average WER:  1.241573271466225\n",
      "duration: 141.38325715065002\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "result = lm3.evaluate('/Users/mehrad/programmings/nlp/codes/phase1/validation_sentences.txt')\n",
    "print(\"average WER: \", result)\n",
    "b = time.time()\n",
    "print(\"duration:\",b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
